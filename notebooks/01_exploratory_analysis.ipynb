import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

# Add the project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))

from src.data.database import (
    get_database_connection, 
    execute_query, 
    get_customer_features,
    get_recent_customer_activity,
    get_customer_segments
)

# Set plotting style
plt.style.use('seaborn-whitegrid')
sns.set_palette('viridis')
plt.rcParams['figure.figsize'] = (12, 8)

# %% [markdown]
# # Customer Churn Prediction: Exploratory Data Analysis
# 
# This notebook performs exploratory data analysis on customer data to identify patterns and factors that may contribute to customer churn.

# %% [markdown]
# ## 1. Load Data from Database

# %%
# Connect to database
print("Connecting to database...")
connection = get_database_connection()
print("Connected successfully!")

# %%
# Get customer features using SQL
print("Extracting customer features from database...")
customer_data = get_customer_features()
print(f"Retrieved {customer_data.shape[0]} customer records with {customer_data.shape[1]} features")

# Display the first few rows
customer_data.head()

# %%
# Get customer segments
customer_segments = get_customer_segments()
print(f"Retrieved {customer_segments.shape[0]} customer segments")

# Display the first few rows
customer_segments.head()

# %%
# Get recent customer activity
recent_activity = get_recent_customer_activity(days=90)
print(f"Retrieved recent activity data for {recent_activity['customer_id'].nunique()} customers")

# Display the first few rows
recent_activity.head()

# %% [markdown]
# ## 2. Data Understanding and Cleaning

# %%
# Check basic statistics
print("Summary statistics:")
customer_data.describe()

# %%
# Check for missing values
print("\nMissing values by column:")
missing_data = customer_data.isnull().sum().sort_values(ascending=False)
missing_percent = (missing_data / len(customer_data)) * 100
missing_summary = pd.concat([missing_data, missing_percent], axis=1, keys=['Count', 'Percent'])
missing_summary[missing_summary['Count'] > 0]

# %%
# Check the distribution of the target variable
churn_distribution = customer_data['churn_status'].value_counts(normalize=True) * 100
print(f"\nChurn Distribution:")
print(f"Not Churned: {churn_distribution[0]:.2f}%")
print(f"Churned: {churn_distribution[1]:.2f}%")

plt.figure(figsize=(8, 6))
sns.countplot(x='churn_status', data=customer_data)
plt.title('Churn Distribution')
plt.xlabel('Churn Status (1 = Churned, 0 = Retained)')
plt.ylabel('Count')
plt.show()

# %% [markdown]
# ## 3. Feature Analysis

# %%
# Analyze numerical features
numerical_cols = customer_data.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_cols = [col for col in numerical_cols if col not in ['customer_id', 'churn_status']]

# Create correlation matrix
correlation_matrix = customer_data[numerical_cols + ['churn_status']].corr()

# Plot correlation with churn
plt.figure(figsize=(12, 10))
churn_correlation = correlation_matrix['churn_status'].sort_values(ascending=False)
sns.barplot(x=churn_correlation.values[1:], y=churn_correlation.index[1:])
plt.title('Correlation of Features with Churn')
plt.xlabel('Correlation Coefficient')
plt.tight_layout()
plt.show()

# Print top positive and negative correlations
print("Top 5 features positively correlated with churn:")
print(churn_correlation[1:6])
print("\nTop 5 features negatively correlated with churn:")
print(churn_correlation[:-6:-1])

# %%
# Plot heatmap of correlation matrix (top features only)
plt.figure(figsize=(16, 14))
top_features = list(churn_correlation.index[1:11]) + list(churn_correlation.index[-10:]) + ['churn_status']
top_features = list(dict.fromkeys(top_features))  # Remove duplicates while preserving order
sns.heatmap(
    customer_data[top_features].corr(), 
    annot=True, 
    cmap='coolwarm', 
    center=0,
    fmt='.2f',
    linewidths=.5
)
plt.title('Correlation Matrix of Top Features')
plt.tight_layout()
plt.show()

# %%
# Analyze categorical features
categorical_cols = customer_data.select_dtypes(include=['object', 'category']).columns.tolist()

for col in categorical_cols:
    if len(customer_data[col].unique()) < 10:  # Only for columns with reasonable number of categories
        plt.figure(figsize=(10, 6))
        churn_by_category = customer_data.groupby(col)['churn_status'].mean().sort_values(ascending=False)
        sns.barplot(x=churn_by_category.index, y=churn_by_category.values)
        plt.title(f'Churn Rate by {col}')
        plt.ylabel('Churn Rate')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
        # Print churn rate by category
        print(f"\nChurn Rate by {col}:")
        category_counts = customer_data[col].value_counts()
        churn_by_category_df = pd.DataFrame({
            'Churn Rate': churn_by_category,
            'Count': category_counts[churn_by_category.index]
        })
        print(churn_by_category_df)

# %% [markdown]
# ## 4. Churn Analysis by Customer Segments

# %%
# Merge customer data with segments
customer_with_segments = pd.merge(
    customer_data, 
    customer_segments[['customer_id', 'customer_segment', 'revenue_quartile', 'usage_quartile']], 
    on='customer_id',
    how='left'
)

# %%
# Analyze churn by customer segment
plt.figure(figsize=(12, 6))
segment_churn = customer_with_segments.groupby('customer_segment')['churn_status'].mean().sort_values(ascending=False)
segment_counts = customer_with_segments['customer_segment'].value_counts()

sns.barplot(x=segment_churn.index, y=segment_churn.values)
plt.title('Churn Rate by Customer Segment')
plt.ylabel('Churn Rate')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Print segment details
print("Churn Rate by Customer Segment:")
segment_churn_df = pd.DataFrame({
    'Churn Rate': segment_churn,
    'Count': segment_counts[segment_churn.index],
    'Percentage': (segment_counts[segment_churn.index] / len(customer_with_segments) * 100).round(2)
})
print(segment_churn_df)

# %%
# Analyze churn by revenue quartile
plt.figure(figsize=(10, 6))
revenue_churn = customer_with_segments.groupby('revenue_quartile')['churn_status'].mean()
sns.barplot(x=revenue_churn.index, y=revenue_churn.values)
plt.title('Churn Rate by Revenue Quartile')
plt.ylabel('Churn Rate')
plt.xlabel('Revenue Quartile (1 = Highest Revenue)')
plt.tight_layout()
plt.show()

# %%
# Analyze churn by usage quartile
plt.figure(figsize=(10, 6))
usage_churn = customer_with_segments.groupby('usage_quartile')['churn_status'].mean()
sns.barplot(x=usage_churn.index, y=usage_churn.values)
plt.title('Churn Rate by Usage Quartile')
plt.ylabel('Churn Rate')
plt.xlabel('Usage Quartile (1 = Highest Usage)')
plt.tight_layout()
plt.show()

# %% [markdown]
# ## 5. Time-based Analysis

# %%
# Convert date columns to datetime
date_columns = ['join_date', 'last_transaction_date', 'first_transaction_date']
for col in date_columns:
    if col in customer_data.columns:
        customer_data[col] = pd.to_datetime(customer_data[col])

# %%
# Analyze churn by customer tenure
customer_data['tenure_months'] = (customer_data['customer_tenure_days'] / 30).astype(int)
tenure_bins = [0, 3, 6, 12, 24, 36, 60, 100000]
tenure_labels = ['0-3 months', '3-6 months', '6-12 months', '1-2 years', '2-3 years', '3-5 years', '5+ years']
customer_data['tenure_group'] = pd.cut(customer_data['tenure_months'], bins=tenure_bins, labels=tenure_labels)

plt.figure(figsize=(12, 6))
tenure_churn = customer_data.groupby('tenure_group')['churn_status'].mean()
tenure_counts = customer_data['tenure_group'].value_counts().sort_index()

sns.barplot(x=tenure_churn.index, y=tenure_churn.values)
plt.title('Churn Rate by Customer Tenure')
plt.ylabel('Churn Rate')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Print tenure details
print("Churn Rate by Customer Tenure:")
tenure_churn_df = pd.DataFrame({
    'Churn Rate': tenure_churn,
    'Count': tenure_counts,
    'Percentage': (tenure_counts / len(customer_data) * 100).round(2)
})
print(tenure_churn_df)

# %%
# Analyze churn by recency of last transaction (days since last transaction)
if 'days_since_last_transaction' in customer_data.columns:
    recency_bins = [0, 7, 30, 60, 90, 180, 365, customer_data['days_since_last_transaction'].max()]
    recency_labels = ['Last week', 'Last month', '1-2 months', '2-3 months', '3-6 months', '6-12 months', '12+ months']
    customer_data['recency_group'] = pd.cut(customer_data['days_since_last
    y_train : pandas.Series or numpy.ndarray
        Training target variable
    X_val : pandas.DataFrame or numpy.ndarray
        Validation features
    y_val : pandas.Series or numpy.ndarray
        Validation target variable
    param_grid : dict, optional
        Grid of hyperparameters to search
    cv : int, optional
        Number of cross-validation folds
        
    Returns:
    --------
    tuple
        (best_model, validation_metrics)
    """
    # Default hyperparameter grid
    if param_grid is None:
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'class_weight': ['balanced', None]
        }
    
    # Initialize base model
    rf = RandomForestClassifier(random_state=42)
    
    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=StratifiedKFold(n_splits=cv),
        scoring='f1',
        n_jobs=-1,
        verbose=1
    )
    
    # Train with grid search
    grid_search.fit(X_train, y_train)
    
    # Get best model
    best_model = grid_search.best_estimator_
    
    # Evaluate on validation set
    val_metrics = evaluate_model(best_model, X_val, y_val, "Random Forest")
    
    # Print best parameters
    print(f"Best Parameters: {grid_search.best_params_}")
    
    return best_model, val_metrics

def train_gradient_boosting(X_train, y_train, X_val, y_val, param_grid=None, cv=5):
    """
    Train a Gradient Boosting classifier with hyperparameter tuning.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame or numpy.ndarray
        Training features
